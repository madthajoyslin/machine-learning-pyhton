1. Problem Definition Techniques

Brainstorming & Stakeholder Interviews ‚Äì conducting the meetings new ideas born (direct contact)
Defining KPIs (Key Performance Indicators) ‚Äì Setting measurable success criteria. 
-Specific, Measurable (Track progress with numbers), Achievable, Relevant, Time-bound (SMART).
------------------------------------------------------------------------------------------------
1Ô∏è‚É£ Specific ‚Üí Clearly define the problem.
‚úÖ "Predict which customers are likely to stop purchasing and send them personalized offers."

2Ô∏è‚É£ Measurable ‚Üí Track progress with numbers.

‚úÖ Example: "Increase model accuracy from 75% to 85%."

confusion matrix: true positive rate

3Ô∏è‚É£ Achievable ‚Üí The goal should be realistic.


‚úÖ Example: "Use past sales data to predict demand with 80% accuracy, not 100% (which is unrealistic)."
4Ô∏è‚É£ Relevant ‚Üí It should relate with the project 

‚úÖ Example: "Detect fraud transactions to reduce financial losses."
5Ô∏è‚É£ Time-bound ‚Üí Set a deadline.

‚úÖ Example: "Deploy the model in 3 months and review results in 6 months."
-------------------------------------------------------------------------------------------


Root Cause Analysis (RCA) ‚Äì Finding the Real Problem
-Whys Technique ‚Äì Keep asking "Why?" to get to the root of the issue.
Understanding  Constraints
-Identify technical constraints (e.g., budget, data availability, time).

--------------------------------------------------------------------------------

2. Data Collection Techniques
a) Surveys & Questionnaires 
Collecting opinions, feedback,

Web Scraping ‚Äì Extracting data from websites (e.g., BeautifulSoup, Scrapy).
-BeautifulSoup reads an HTML/XML document and converts it into a structured format that we can easily search and manipulate.

import requests
from bs4 import BeautifulSoup

# Step 1: Get the webpage content
url = "https://example.com"  # Sample website
response = requests.get(url)  # Send request to the website

# Step 2: Parse the HTML
soup = BeautifulSoup(response.text, "html.parser")

# Step 3: Extract data
title = soup.title.text  # Get the title of the webpage
heading = soup.h1.text  # Get the first <h1> heading

# Step 4: Print the extracted data
print(f"Page Title: {title}")
print(f"Main Heading: {heading}")


API Integration ‚Äì Fetching data from services (e.g., Twitter API for sentiment analysis).
----------------------------------------------------------------------------------------
for company details

APIs provide direct access to data from platforms like Twitter, Google Maps, OpenWeather, etc.
The data is structured and easy to use (JSON/XML). better than web scraping
Requires API keys for authentication.

import requests

api_key = "your_api_key"
city = "New York"
url = f"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={api_key}"

response = requests.get(url)
data = response.json()

print("Weather:", data["weather"][0]["description"])
--------------------------------------------------------------
Pre-existing Data
Database Queries ‚Äì Using SQL to retrieve data from databases
-------------------------------------------------------------
using kaggle is free

CSV, XLS, PDF, 


3. Data Cleaning & Preprocessing Techniques
Handling Missing Values ‚Äì Using imputation (mean, median) or dropping records.
---------------------------------------------------------------------------------
Techniques:

Removal: Delete rows/columns with missing values (if they are insignificant).

Example:

ID	Name		Age	Salary
1	Alex		25	50000
2	Bob		NaN	60000
3	Charlie	30	NaN
üîπ Filling Age with the mean and Salary with median ensures no missing values.
Mean (Average)	When data is normally distributed (no extreme values/outliers).
Median (Middle Value)	When data has outliers (like salaries with very high/low values).



--------------------------------------------------------------------------
Removing Duplicates (not suggessted)
Use duplicated() to find duplicate values.
Use drop_duplicates() to remove them.
Use subset=['column_name'] to remove duplicates based on a specific column.
Use reset_index(drop=True) to keep the dataset clean
-------------------------------------------------------

 Outliers(extreme values) ‚Äì Using Z-score
Z-Score Method ‚Üí Remove data points beyond ¬±3 standard deviations

import numpy as np
import pandas as pd
from scipy.stats import zscore

# Sample dataset
data = {'Values': [10, 12, 14, 15, 100, 16, 13, 11, 10, 12, 150]}
df = pd.DataFrame(data)

# Calculate Z-scores
df['Z-score'] = np.abs(zscore(df['Values']))

# Filter out outliers (keep values with Z-score ‚â§ 3)
df_cleaned = df[df['Z-score'] <= 3]

# Drop Z-score column and display cleaned data
df_cleaned = df_cleaned.drop(columns=['Z-score'])
print(df_cleaned)
Encoding Categorical Data ‚Äì One-hot encoding or label encoding.

---------------------------------------------------------
4. Exploratory Data Analysis (EDA) Techniques
Visualization Tools ‚Äì Matplotlib, Seaborn for insights.
Correlation Analysis ‚Äì Heatmaps to find relationships.
Statistical Summary ‚Äì Mean, median, standard deviation, etc.
Example: An insurance company analyzes claims data to detect fraud trends.
-------------------------------------------------------------------------------
Data processing
Techniques to handle missing vales
------------------------------------------------------------------------------

a) Mean / Median / Mode Imputation
Mean for numerical data (if normally distributed).
Median for numerical data (if skewed).(outlier)
Mode for categorical data.


import pandas as pd
import numpy as np

# Creating a sample DataFrame with missing values
data = {
    'Age': [25, 30, np.nan, 35, 40, np.nan, 28],
    'Salary': [50000, 60000, 70000, np.nan, 80000, 90000, np.nan],
    'City': ['New York', 'Los Angeles', np.nan, 'Chicago', 'New York', 'Chicago', np.nan]
}

df = pd.DataFrame(data)

# Display the original DataFrame
print("Original DataFrame:")
print(df)

# Handling missing values
df['Age'].fillna(df['Age'].mean(), inplace=True)     
df['Salary'].fillna(df['Salary'].median(), inplace=True)  
df['City'].fillna(df['City'].mode()[0], inplace=True)

# Display the DataFrame after filling missing values
print("\nDataFrame after handling missing values:")
print(df)

--------------------------------------------------------------------------------------------
 K-Nearest Neighbors (KNN) Imputation
-------------------------------------------------------------------------------------------
Replaces missing values with the average of nearest neighbors.
here k=3

import pandas as pd
import numpy as np
from sklearn.impute import KNNImputer

# Creating a sample DataFrame with missing values
data = {
    'Age': [25, 30, np.nan, 35, 40, np.nan, 28],
    'Salary': [50000, 60000, 70000, np.nan, 80000, 90000, np.nan],
    'Experience': [1, 3, 5, np.nan, 10, 12, np.nan]  # Additional column for better KNN impact
}

df = pd.DataFrame(data)

# Display the original DataFrame
print("Original DataFrame with Missing Values:")
print(df)

# Initialize KNN Imputer (using k=3 for nearest neighbors)
imputer = KNNImputer(n_neighbors=3)

# Apply imputation
df_filled = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)

# Display the DataFrame after KNN Imputation
print("\nDataFrame after KNN Imputation:")
print(df_filled)
-------------------------------------------------------------------------
Linear regression 
based on salary


import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression

# Create a small dataset with missing Age values
data = {
    'Age': [25, 30, np.nan, 35, 40, np.nan],  # Some missing values (dependent)
    'Salary': [50000, 60000, 70000, 80000, 90000, 100000]
}

df = pd.DataFrame(data)

# Separate complete and missing data
df_complete = df.dropna()  # Rows where Age is not NaN
df_missing = df[df['Age'].isnull()]  # Rows where Age is NaN

# Train Linear Regression model
model = LinearRegression()
model.fit(df_complete[['Salary']], df_complete['Age'])

# Predict missing Age values
df.loc[df['Age'].isnull(), 'Age'] = model.predict(df_missing[['Salary']])

# Print the updated DataFrame
print(df)
------------------------------------------------------------------
Outliers
-------------------------------------------------------------------
Binning techniques
Binning is useful when outliers exist but grouping data into categories makes sense.

import pandas as pd

# Sample dataset
data = {'Age': [5, 18, 25, 35, 45, 55, 65, 75, 85, 95]}
df = pd.DataFrame(data)

# Define bins and labels
bins = [0, 30, 50, 120]  # Age ranges: 0-30, 31-50, 51-120
labels = ['Young', 'Middle-aged', 'Senior']

# Apply binning
df['Age_Group'] = pd.cut(df['Age'], bins=bins, labels=labels)

# Print results
print(df)


---------------------------------------------------------------------
Transforming Data to Log
-------------------------------------------------------------------------
import numpy as np
import pandas as pd

# Sample dataset
data = {'Age': [18, 22, 25, 30, 35, 40, 100, 110, 120]}  # 100, 110, 120 are outliers
df = pd.DataFrame(data)

# Apply Log Transformation
df['Age_Log'] = np.log(df['Age'] + 1)  # Adding 1 to avoid log(0)

# Print the transformed data
print(df)



---------------------------------------------------------------------
Replacing Outliers Using Capping (Winsorization) 

If a value is too high, we replace it with the upper bound.
If a value is too low, we replace it with the lower bound.

import numpy as np
import pandas as pd

# Sample dataset with outliers
data = {'Age': [25, 30, 35, 40, 100, 28, 32, 29, 110]}  # 100 & 110 are outliers
df = pd.DataFrame(data)

# Step 1: Calculate IQR (Interquartile Range)
Q1 = df['Age'].quantile(0.25)  # 25th percentile
Q3 = df['Age'].quantile(0.75)  # 75th percentile
IQR = Q3 - Q1  # Interquartile Range

# Step 2: Define lower and upper bounds for outliers
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Step 3: Replace outliers with the nearest valid limit (Winsorization)
df['Age'] = np.where(df['Age'] > upper_bound, upper_bound, df['Age'])
df['Age'] = np.where(df['Age'] < lower_bound, lower_bound, df['Age'])

# Print the modified dataset
print(df)
-------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------





